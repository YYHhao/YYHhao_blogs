**逻辑回归与神经网络的关系：**   
逻辑回归可以看作是只有个神经元的单层神经网络，神经网络就是一组神经元连接在一起的网络，隐藏层可以有多个神经元，每个神经元本身就对应着一个逻辑回归过程。

**神经网络的权重不能全为0的原因：**    
神经网络的权重w 的不同代表输入的向量有不同的特征，即权重越大的特征越重要，比如在人脸识别中，人脸的属性有眼睛，鼻子，嘴巴，眉毛，其中眼睛更能够影响人脸的识别，所以我们给与眼睛更大的权重。

如果将权重初始化全为0，那么隐藏层的各个神经元的结果都是一样的，从而正向传播的结果是一样的，反向传播求得的梯度也是一样的，也就是说不管经过多少次迭代，更新的w(i)是相同的，这样就判断不了哪个特征比较重要了。

因此，初始w不同，可以学到不同的特征，如果都是0或某个值，由于计算方式相同，可能达不到学习不同特征的目的。

**逻辑回归的权重可以初始化为0的原因：**    
Logistic回归没有隐藏层。 如果将权重初始化为零，则Logistic回归中的第一个示例x将输出零，但Logistic回归的导数取决于不是零的输入x（因为没有隐藏层）。 因此，在第二次迭代（迭代发生在w和b值的更新中，即梯度下降）中，如果x不是常量向量，则权值遵循x的分布并且彼此不同。

# 实现   
W:权重矩阵，维度为（layers_dims[1], layers_dims[0]）    
1、零初始化    
```python
parameters = {}  
L = len(layers_dims) #网络层数    
for l in range(1,L):
    parameters["W" + str(l)] = np.zeros((layers_dims[l],layers_dims[l-1]))  
```  

2、随机初始化    
```python
parameters["W" + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*10  
```   

3、抑梯度异常初始化    
```python
parameters["W" + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2/layers_dims[l-1]) 
```
